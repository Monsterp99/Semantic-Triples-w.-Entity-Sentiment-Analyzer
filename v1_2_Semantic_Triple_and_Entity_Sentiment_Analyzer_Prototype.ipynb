{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kcGF0PjWBkS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Drive already mounted\")\n",
        "\n",
        "# =====================================================\n",
        "# INDUSTRY TEMPLATES LIBRARY\n",
        "# =====================================================\n",
        "\n",
        "class IndustryTemplates:\n",
        "    \"\"\"Pre-built configuration templates for different industries\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_academic_template():\n",
        "        \"\"\"Academic/Education industry template (placeholder example)\"\"\"\n",
        "        return {\n",
        "            \"template_name\": \"Academic/Education\",\n",
        "            \"description\": \"Template for universities, colleges, and educational institutions\",\n",
        "            \"categories\": {\n",
        "                \"academic_programs\": {\n",
        "                    \"weight\": 0.25,\n",
        "                    \"display_name\": \"Academic Programs\",\n",
        "                    \"description\": \"Degree programs, courses, curriculum offerings\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(degree programs?|courses?|curriculum|majors?|minors?)\\s+(offers?|includes?|provides?|features?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(bachelor|master|doctoral|PhD|certificate)\\s+(in|of|programs?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(academic programs?|educational offerings?|study options?)\\s+(include|encompasses?|covers?)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"degree\", \"program\", \"curriculum\", \"courses\", \"major\", \"minor\", \"bachelor\", \"master\", \"PhD\"]\n",
        "                },\n",
        "                \"student_outcomes\": {\n",
        "                    \"weight\": 0.30,\n",
        "                    \"display_name\": \"Student Outcomes\",\n",
        "                    \"description\": \"Graduation rates, career placement, alumni success\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(graduation rate|employment rate|job placement|career outcomes?)\\s+(of|is|reaches?|achieves?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(students?|graduates?|alumni)\\s+(achieve|secure|obtain|find)\\s+([^.]{1,60})',\n",
        "                        r'\\b(success rate|placement rate|outcome statistics?)\\s+(shows?|indicates?|demonstrates?)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"graduation\", \"employment\", \"career\", \"alumni\", \"success\", \"placement\", \"outcomes\"]\n",
        "                },\n",
        "                \"accreditation\": {\n",
        "                    \"weight\": 0.20,\n",
        "                    \"display_name\": \"Accreditation & Quality\",\n",
        "                    \"description\": \"Accreditation status, certifications, quality indicators\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(accredited by|certified by|approved by|recognized by)\\s+([^.]{1,60})',\n",
        "                        r'\\b(accreditation|certification|approval|recognition)\\s+(from|by|ensures?|guarantees?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(quality|standards?|excellence)\\s+(maintained|upheld|demonstrated)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"accredited\", \"certified\", \"approved\", \"recognition\", \"quality\", \"standards\", \"excellence\"]\n",
        "                },\n",
        "                \"student_support\": {\n",
        "                    \"weight\": 0.15,\n",
        "                    \"display_name\": \"Student Support\",\n",
        "                    \"description\": \"Services and support for students\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(for students?|student services?|support services?)\\s+(includes?|offers?|provides?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(students?)\\s+(receive|access|benefit from|enjoy)\\s+([^.]{1,60})',\n",
        "                        r'\\b(academic support|tutoring|counseling|advising)\\s+(helps?|assists?|guides?)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"students\", \"support\", \"services\", \"tutoring\", \"counseling\", \"advising\", \"assistance\"]\n",
        "                },\n",
        "                \"differentiators\": {\n",
        "                    \"weight\": 0.10,\n",
        "                    \"display_name\": \"Differentiators\",\n",
        "                    \"description\": \"Competitive advantages and unique features\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(unlike other universities?|compared to other schools?|different from traditional)\\s+([^.]{1,80})',\n",
        "                        r'\\b(only|exclusively|unique|distinctive|innovative)\\s+([^.]{1,60})',\n",
        "                        r'\\b(our university|our institution|our approach)\\s+(is the only|uniquely|exclusively)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"unique\", \"innovative\", \"distinctive\", \"exclusive\", \"different\", \"special\"]\n",
        "                }\n",
        "            },\n",
        "            \"competitors\": [\"harvard university\", \"stanford university\", \"mit\", \"yale university\", \"princeton\"],\n",
        "            \"compliance_terms\": [\"accredited\", \"licensed\", \"approved\", \"certified\", \"recognized\"],\n",
        "            \"priority_thresholds\": {\n",
        "                \"high\": 0.3,\n",
        "                \"medium\": 0.6\n",
        "            },\n",
        "            \"noise_patterns\": [\n",
        "                \"student portal\", \"login\", \"apply now\", \"request information\",\n",
        "                \"campus tour\", \"virtual tour\", \"contact admissions\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_healthcare_template():\n",
        "        \"\"\"Healthcare/Medical industry template\"\"\"\n",
        "        return {\n",
        "            \"template_name\": \"Healthcare/Medical\",\n",
        "            \"description\": \"Template for hospitals, clinics, and medical practices\",\n",
        "            \"categories\": {\n",
        "                \"medical_services\": {\n",
        "                    \"weight\": 0.25,\n",
        "                    \"display_name\": \"Medical Services\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(treatments?|procedures?|surgeries?|therapies?)\\s+(offers?|provides?|performs?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(medical services?|healthcare services?)\\s+(include|encompasses?)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"treatment\", \"procedure\", \"surgery\", \"therapy\", \"medical\", \"healthcare\"]\n",
        "                },\n",
        "                \"patient_outcomes\": {\n",
        "                    \"weight\": 0.30,\n",
        "                    \"display_name\": \"Patient Outcomes\",\n",
        "                    \"patterns\": [\n",
        "                        r'\\b(success rate|recovery rate|patient satisfaction)\\s+(of|is|reaches?)\\s+([^.]{1,60})',\n",
        "                        r'\\b(patients?)\\s+(recover|improve|heal|benefit)\\s+([^.]{1,60})'\n",
        "                    ],\n",
        "                    \"entities\": [\"success\", \"recovery\", \"satisfaction\", \"patients\", \"outcomes\", \"results\"]\n",
        "                }\n",
        "                # ... other healthcare categories\n",
        "            },\n",
        "            \"competitors\": [\"mayo clinic\", \"cleveland clinic\", \"johns hopkins\"],\n",
        "            \"compliance_terms\": [\"FDA approved\", \"board certified\", \"licensed\", \"accredited\"],\n",
        "            \"priority_thresholds\": {\"high\": 0.3, \"medium\": 0.6}\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_templates():\n",
        "        \"\"\"Get list of all available templates\"\"\"\n",
        "        return {\n",
        "            \"academic\": IndustryTemplates.get_academic_template(),\n",
        "            \"healthcare\": IndustryTemplates.get_healthcare_template()\n",
        "            # Add more templates as needed\n",
        "        }\n",
        "\n",
        "# =====================================================\n",
        "# CONFIGURATION MANAGER\n",
        "# =====================================================\n",
        "\n",
        "class ConfigurationManager:\n",
        "    \"\"\"Manages client configurations and Drive integration\"\"\"\n",
        "\n",
        "    def __init__(self, base_drive_path=\"SemanticTool_V2\"):\n",
        "        self.base_path = f\"/content/drive/MyDrive/{base_drive_path}\"\n",
        "        self.templates_path = f\"{self.base_path}/Templates\"\n",
        "        self.configs_path = f\"{self.base_path}/Configs\"\n",
        "        self.analyses_path = f\"{self.base_path}/Analyses\"\n",
        "\n",
        "        # Create directory structure\n",
        "        self._create_directory_structure()\n",
        "\n",
        "    def _create_directory_structure(self):\n",
        "        \"\"\"Create necessary directories in Google Drive\"\"\"\n",
        "        directories = [\n",
        "            self.base_path,\n",
        "            self.templates_path,\n",
        "            self.configs_path,\n",
        "            self.analyses_path\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        print(f\"âœ… Directory structure created in: {self.base_path}\")\n",
        "\n",
        "    def save_template(self, template_name, template_config):\n",
        "        \"\"\"Save a template configuration to Drive\"\"\"\n",
        "        file_path = f\"{self.templates_path}/{template_name}.json\"\n",
        "\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(template_config, f, indent=2)\n",
        "\n",
        "        print(f\"âœ… Template saved: {template_name}.json\")\n",
        "\n",
        "    def load_template(self, template_name):\n",
        "        \"\"\"Load a template configuration from Drive\"\"\"\n",
        "        file_path = f\"{self.templates_path}/{template_name}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âŒ Template not found: {template_name}.json\")\n",
        "            return None\n",
        "\n",
        "    def create_client_config(self, client_name, template_name=None, custom_config=None):\n",
        "        \"\"\"Create a client-specific configuration\"\"\"\n",
        "\n",
        "        if custom_config:\n",
        "            config = custom_config\n",
        "        elif template_name:\n",
        "            # Load from template\n",
        "            if template_name in IndustryTemplates.get_available_templates():\n",
        "                config = IndustryTemplates.get_available_templates()[template_name]\n",
        "            else:\n",
        "                config = self.load_template(template_name)\n",
        "                if not config:\n",
        "                    return None\n",
        "        else:\n",
        "            print(\"âŒ Must provide either template_name or custom_config\")\n",
        "            return None\n",
        "\n",
        "        # Add client-specific metadata\n",
        "        client_config = {\n",
        "            \"client_name\": client_name,\n",
        "            \"created_date\": datetime.now().isoformat(),\n",
        "            \"template_used\": template_name,\n",
        "            \"file_naming\": {\n",
        "                \"analysis_prefix\": f\"{client_name}_{datetime.now().strftime('%Y_%m')}\",\n",
        "                \"folder_structure\": f\"{client_name}_{datetime.now().strftime('%Y_%m')}\"\n",
        "            },\n",
        "            **config\n",
        "        }\n",
        "\n",
        "        # Save client config\n",
        "        config_file = f\"{self.configs_path}/{client_name}_config.json\"\n",
        "        with open(config_file, 'w') as f:\n",
        "            json.dump(client_config, f, indent=2)\n",
        "\n",
        "        print(f\"âœ… Client configuration created: {client_name}_config.json\")\n",
        "        return client_config\n",
        "\n",
        "    def load_client_config(self, client_name):\n",
        "        \"\"\"Load client configuration from Drive\"\"\"\n",
        "        config_file = f\"{self.configs_path}/{client_name}_config.json\"\n",
        "\n",
        "        try:\n",
        "            with open(config_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âŒ Client config not found: {client_name}_config.json\")\n",
        "            return None\n",
        "\n",
        "    def create_analysis_folder(self, client_config):\n",
        "        \"\"\"Create organized folder structure for analysis\"\"\"\n",
        "        folder_name = client_config[\"file_naming\"][\"folder_structure\"]\n",
        "        analysis_folder = f\"{self.analyses_path}/{folder_name}\"\n",
        "\n",
        "        # Create subfolders\n",
        "        subfolders = [\"input_files\", \"results\", \"visualizations\", \"exports\"]\n",
        "\n",
        "        for subfolder in subfolders:\n",
        "            os.makedirs(f\"{analysis_folder}/{subfolder}\", exist_ok=True)\n",
        "\n",
        "        print(f\"âœ… Analysis folder created: {folder_name}\")\n",
        "        return analysis_folder\n",
        "\n",
        "    def list_available_configs(self):\n",
        "        \"\"\"List all available client configurations\"\"\"\n",
        "        try:\n",
        "            configs = [f for f in os.listdir(self.configs_path) if f.endswith('_config.json')]\n",
        "            return [f.replace('_config.json', '') for f in configs]\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "# =====================================================\n",
        "# CONFIGURATION SETUP FUNCTIONS\n",
        "# =====================================================\n",
        "\n",
        "def setup_configuration_system():\n",
        "    \"\"\"Initialize the configuration system\"\"\"\n",
        "    print(\"ðŸš€ Setting up V2 Configuration System\")\n",
        "\n",
        "    # Initialize configuration manager\n",
        "    config_manager = ConfigurationManager()\n",
        "\n",
        "    # Save industry templates to Drive\n",
        "    templates = IndustryTemplates.get_available_templates()\n",
        "    for template_name, template_config in templates.items():\n",
        "        config_manager.save_template(template_name, template_config)\n",
        "\n",
        "    print(\"âœ… Configuration system ready!\")\n",
        "    return config_manager\n",
        "\n",
        "def create_new_client(client_name, industry_template=\"academic\", config_manager=None):\n",
        "    \"\"\"Create a new client configuration using academic template as example\"\"\"\n",
        "\n",
        "    if not config_manager:\n",
        "        config_manager = ConfigurationManager()\n",
        "\n",
        "    print(f\"ðŸ“‹ Creating client configuration: {client_name}\")\n",
        "    print(f\"ðŸ“‹ Using template: {industry_template}\")\n",
        "\n",
        "    # Create client config\n",
        "    client_config = config_manager.create_client_config(\n",
        "        client_name=client_name,\n",
        "        template_name=industry_template\n",
        "    )\n",
        "\n",
        "    # Create analysis folders\n",
        "    analysis_folder = config_manager.create_analysis_folder(client_config)\n",
        "\n",
        "    print(f\"âœ… Client setup complete: {client_name}\")\n",
        "    print(f\"ðŸ“ Analysis folder: {analysis_folder}\")\n",
        "\n",
        "    return client_config, analysis_folder, config_manager\n",
        "\n",
        "def modify_client_config(client_name, modifications, config_manager=None):\n",
        "    \"\"\"Modify existing client configuration\"\"\"\n",
        "\n",
        "    if not config_manager:\n",
        "        config_manager = ConfigurationManager()\n",
        "\n",
        "    # Load existing config\n",
        "    config = config_manager.load_client_config(client_name)\n",
        "    if not config:\n",
        "        return None\n",
        "\n",
        "    # Apply modifications\n",
        "    for key, value in modifications.items():\n",
        "        if key in config:\n",
        "            config[key] = value\n",
        "        else:\n",
        "            print(f\"âš ï¸ Key '{key}' not found in config\")\n",
        "\n",
        "    # Save updated config\n",
        "    config_file = f\"{config_manager.configs_path}/{client_name}_config.json\"\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"âœ… Configuration updated for: {client_name}\")\n",
        "    return config\n",
        "\n",
        "# =====================================================\n",
        "# EXAMPLE USAGE & TESTING\n",
        "# =====================================================\n",
        "\n",
        "def demo_configuration_system():\n",
        "    \"\"\"Demonstrate the configuration system with academic examples\"\"\"\n",
        "\n",
        "    print(\"ðŸŽ“ DEMO: Academic Institution Configuration\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Setup system\n",
        "    config_manager = setup_configuration_system()\n",
        "\n",
        "    # Create example academic client\n",
        "    client_config, analysis_folder, config_manager = create_new_client(\n",
        "        client_name=\"StateUniversity_VetSchool\",\n",
        "        industry_template=\"academic\",\n",
        "        config_manager=config_manager\n",
        "    )\n",
        "\n",
        "    # Show configuration details\n",
        "    print(\"\\nðŸ“Š Client Configuration Summary:\")\n",
        "    print(f\"Client: {client_config['client_name']}\")\n",
        "    print(f\"Template: {client_config['template_used']}\")\n",
        "    print(f\"Categories: {len(client_config['categories'])}\")\n",
        "\n",
        "    for cat_name, cat_config in client_config['categories'].items():\n",
        "        print(f\"  - {cat_config['display_name']}: {cat_config['weight']}\")\n",
        "\n",
        "    print(f\"\\nðŸ“ File Structure:\")\n",
        "    print(f\"Base Path: {config_manager.base_path}\")\n",
        "    print(f\"Analysis Folder: {analysis_folder}\")\n",
        "\n",
        "    return config_manager, client_config\n",
        "\n",
        "# =====================================================\n",
        "# READY TO USE\n",
        "# =====================================================\n",
        "\n",
        "print(\"âœ… V2 Configuration System Loaded!\")\n",
        "print(\"\\nQuick Start:\")\n",
        "print(\"1. config_manager, client_config = demo_configuration_system()\")\n",
        "print(\"2. Or create custom: config_manager = setup_configuration_system()\")\n",
        "print(\"3. Then: client_config, folder, manager = create_new_client('YourClient', 'academic')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: V2 Semantic Analyzer\n",
        "\n",
        "- Configuration-Driven Analysis - Uses client config for categories, weights, patterns\n",
        "\n",
        "- Auto Entity Detection - Automatically uses entity data if available,\n",
        "gracefully falls back if not\n",
        "\n",
        "- Enhanced Content Cleaning - Robust cleaning system with configuration-aware filters\n",
        "\n",
        "- Weighted Scoring - Category scores use configured weights\n",
        "\n",
        "- Complete Drive Integration - All file handling through Google Drive\n",
        "\n",
        "- Comprehensive Results - Detailed analysis with entity enhancement indicators"
      ],
      "metadata": {
        "id": "KO18EOFlXG1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =====================================================\n",
        "# V2 ENHANCED SEMANTIC ANALYZER\n",
        "# =====================================================\n",
        "\n",
        "class SemanticAnalyzerV2:\n",
        "    \"\"\"Version 2 Semantic Analyzer with configuration-driven analysis\"\"\"\n",
        "\n",
        "    def __init__(self, client_config):\n",
        "        \"\"\"Initialize analyzer with client configuration\"\"\"\n",
        "        self.config = client_config\n",
        "        self.client_name = client_config['client_name']\n",
        "        self.categories = client_config['categories']\n",
        "        self.competitors = client_config.get('competitors', [])\n",
        "        self.compliance_terms = client_config.get('compliance_terms', [])\n",
        "        self.priority_thresholds = client_config['priority_thresholds']\n",
        "        self.noise_patterns = client_config.get('noise_patterns', [])\n",
        "\n",
        "        # Entity analysis will be detected automatically\n",
        "        self.entity_data = None\n",
        "        self.entity_enhanced = False\n",
        "\n",
        "        print(f\"ðŸŽ¯ Analyzer initialized for: {self.client_name}\")\n",
        "        print(f\"ðŸ“Š Categories configured: {len(self.categories)}\")\n",
        "        print(f\"ðŸ† Entity enhancement: Will auto-detect\")\n",
        "\n",
        "    def load_entity_data(self, entity_file_path):\n",
        "        \"\"\"Load and clean entity analysis data (optional)\"\"\"\n",
        "        try:\n",
        "            print(f\"ðŸ” Attempting to load entity data from: {entity_file_path}\")\n",
        "\n",
        "            # Load entity data\n",
        "            entity_df = pd.read_excel(entity_file_path, sheet_name='Entity Sentiment Data')\n",
        "\n",
        "            # Clean entity data - remove noise entities\n",
        "            base_noise_entities = ['cookies', 'site', 'privacy', 'policy', 'terms', 'website', 'page', 'javascript']\n",
        "            config_noise_entities = [term.lower() for term in self.noise_patterns]\n",
        "            all_noise_entities = base_noise_entities + config_noise_entities\n",
        "\n",
        "            cleaned_entities = entity_df[~entity_df['Entity'].str.lower().isin(all_noise_entities)]\n",
        "\n",
        "            # Filter by minimum salience\n",
        "            meaningful_entities = cleaned_entities[cleaned_entities['Salience'] > 0.01]\n",
        "\n",
        "            self.entity_data = meaningful_entities\n",
        "            self.entity_enhanced = True\n",
        "\n",
        "            print(f\"âœ… Entity analysis loaded: {len(meaningful_entities)} meaningful entities\")\n",
        "            print(f\"ðŸŽ¯ Entity enhancement: ENABLED\")\n",
        "\n",
        "            return meaningful_entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Entity data not available: {str(e)}\")\n",
        "            print(f\"ðŸ“ Proceeding with text-only analysis\")\n",
        "            self.entity_enhanced = False\n",
        "            return None\n",
        "\n",
        "    def get_page_entities(self, url):\n",
        "        \"\"\"Get entities for a specific page\"\"\"\n",
        "        if self.entity_data is None:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Filter entities for this specific URL\n",
        "        page_entities = self.entity_data[self.entity_data['ID'] == url].copy()\n",
        "        return page_entities\n",
        "\n",
        "    def clean_content(self, content):\n",
        "        \"\"\"Enhanced content cleaning with configuration-aware filters\"\"\"\n",
        "        if not content:\n",
        "            return \"\"\n",
        "\n",
        "        # Base noise patterns\n",
        "        base_patterns = [\n",
        "            r'cookies?.*?privacy.*?policy',\n",
        "            r'javascript.*?enabled',\n",
        "            r'website.*?functionality',\n",
        "            r'terms.*?conditions',\n",
        "            r'privacy.*?policy',\n",
        "            r'site.*?navigation',\n",
        "            r'accept.*?cookies',\n",
        "            r'cookie.*?settings',\n",
        "            r'we use cookies',\n",
        "            r'this website uses',\n",
        "            r'by continuing to use'\n",
        "        ]\n",
        "\n",
        "        # Add configuration-specific noise patterns\n",
        "        config_patterns = [\n",
        "            rf'\\b{re.escape(term.lower())}\\b.*?[\\.\\!\\?]'\n",
        "            for term in self.noise_patterns\n",
        "        ]\n",
        "\n",
        "        all_patterns = base_patterns + config_patterns\n",
        "\n",
        "        # Clean content\n",
        "        cleaned = content\n",
        "        for pattern in all_patterns:\n",
        "            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "        # Enhanced UI element removal\n",
        "        ui_patterns = [\n",
        "            r'These cookies.*?understand how users interact with[^.]*\\.',\n",
        "            r'We use cookies.*?improve.*?performance[^.]*\\.',\n",
        "            r'By clicking.*?accept.*?cookies[^.]*\\.',\n",
        "            r'Footer.*?navigation.*?menu[^.]*\\.',\n",
        "            r'Header.*?logo.*?menu[^.]*\\.',\n",
        "            r'Menu.*?navigation.*?links[^.]*\\.',\n",
        "            r'Contact us.*?for more information[^.]*\\.',\n",
        "            r'Click here.*?to learn more[^.]*\\.'\n",
        "        ]\n",
        "\n",
        "        for pattern in ui_patterns:\n",
        "            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def extract_category_triples(self, content, category_name, page_entities=None):\n",
        "        \"\"\"Extract semantic triples for a specific category using configuration\"\"\"\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        category_config = self.categories[category_name]\n",
        "        triples = []\n",
        "\n",
        "        # Extract triples using configured patterns\n",
        "        for pattern in category_config['patterns']:\n",
        "            matches = re.finditer(pattern, content, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                groups = match.groups()\n",
        "                if len(groups) >= 3:\n",
        "                    subject = groups[0].strip()\n",
        "                    predicate = groups[1].strip() if len(groups) > 1 else 'relates_to'\n",
        "                    obj = groups[2].strip()\n",
        "\n",
        "                    # Clean up the extracted parts\n",
        "                    subject = re.sub(r'\\s+', ' ', subject)\n",
        "                    obj = re.sub(r'\\s+', ' ', obj)\n",
        "\n",
        "                    # Calculate base confidence\n",
        "                    base_confidence = self._calculate_triple_confidence(\n",
        "                        subject, predicate, obj, category_name\n",
        "                    )\n",
        "\n",
        "                    triple = {\n",
        "                        'category': category_name,\n",
        "                        'subject': subject,\n",
        "                        'predicate': predicate,\n",
        "                        'object': obj,\n",
        "                        'confidence': base_confidence,\n",
        "                        'pattern_match': match.group(0)[:100] + '...' if len(match.group(0)) > 100 else match.group(0)\n",
        "                    }\n",
        "\n",
        "                    # Enhance confidence with entity analysis if available\n",
        "                    if page_entities is not None and not page_entities.empty:\n",
        "                        triple['confidence'] = self._enhance_triple_confidence_with_entities(\n",
        "                            triple, page_entities\n",
        "                        )\n",
        "                        triple['entity_enhanced'] = True\n",
        "                    else:\n",
        "                        triple['entity_enhanced'] = False\n",
        "\n",
        "                    triples.append(triple)\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _calculate_triple_confidence(self, subject, predicate, obj, category_name):\n",
        "        \"\"\"Calculate base confidence score using configuration\"\"\"\n",
        "        confidence = 0.5  # Base confidence\n",
        "\n",
        "        # Bonus for category-specific entities\n",
        "        category_entities = self.categories[category_name].get('entities', [])\n",
        "        for entity in category_entities:\n",
        "            if entity.lower() in subject.lower() or entity.lower() in obj.lower():\n",
        "                confidence += 0.2\n",
        "                break\n",
        "\n",
        "        # Bonus for strong predicates\n",
        "        strong_predicates = ['provides', 'offers', 'helps', 'solves', 'eliminates', 'lasts', 'includes', 'achieves', 'delivers']\n",
        "        if predicate.lower() in strong_predicates:\n",
        "            confidence += 0.15\n",
        "\n",
        "        # Bonus for client-specific terms\n",
        "        client_terms = [self.client_name.lower().split('_')[0]]  # Extract base client name\n",
        "        if any(term in subject.lower() for term in client_terms):\n",
        "            confidence += 0.15\n",
        "\n",
        "        # Bonus for compliance terms\n",
        "        if any(term.lower() in subject.lower() or term.lower() in obj.lower()\n",
        "               for term in self.compliance_terms):\n",
        "            confidence += 0.1\n",
        "\n",
        "        # Penalty for very long or very short objects\n",
        "        if len(obj) < 10 or len(obj) > 80:\n",
        "            confidence -= 0.1\n",
        "\n",
        "        return min(1.0, max(0.1, confidence))\n",
        "\n",
        "    def _enhance_triple_confidence_with_entities(self, triple, page_entities):\n",
        "        \"\"\"Use entity analysis to enhance triple confidence scores\"\"\"\n",
        "        enhanced_confidence = triple['confidence']\n",
        "\n",
        "        # Look for subject entity in entity analysis\n",
        "        subject_entities = page_entities[\n",
        "            page_entities['Entity'].str.lower().str.contains(triple['subject'].lower(), na=False)\n",
        "        ]\n",
        "\n",
        "        if not subject_entities.empty:\n",
        "            # Boost confidence based on entity salience\n",
        "            max_salience = subject_entities['Salience'].max()\n",
        "            salience_boost = min(0.2, max_salience * 2)\n",
        "            enhanced_confidence += salience_boost\n",
        "\n",
        "            # Check sentiment alignment\n",
        "            avg_sentiment = subject_entities['Sentiment Score'].mean()\n",
        "            if triple['category'] in ['student_outcomes', 'accreditation', 'academic_programs'] and avg_sentiment > 0:\n",
        "                enhanced_confidence += 0.1\n",
        "            elif triple['category'] == 'differentiators' and avg_sentiment > 0:\n",
        "                enhanced_confidence += 0.1\n",
        "\n",
        "        # Look for object entity\n",
        "        object_entities = page_entities[\n",
        "            page_entities['Entity'].str.lower().str.contains(triple['object'].lower(), na=False)\n",
        "        ]\n",
        "\n",
        "        if not object_entities.empty:\n",
        "            max_salience = object_entities['Salience'].max()\n",
        "            salience_boost = min(0.15, max_salience * 1.5)\n",
        "            enhanced_confidence += salience_boost\n",
        "\n",
        "        return min(1.0, enhanced_confidence)\n",
        "\n",
        "    def analyze_entity_quality(self, url, page_entities):\n",
        "        \"\"\"Analyze entity quality issues for a page\"\"\"\n",
        "        if page_entities.empty:\n",
        "            return {\n",
        "                'total_entities': 0,\n",
        "                'entity_issues': [],\n",
        "                'entity_quality_score': 0,\n",
        "                'top_entities': []\n",
        "            }\n",
        "\n",
        "        issues = []\n",
        "\n",
        "        # Check for high-salience noise entities\n",
        "        high_salience_entities = page_entities[page_entities['Salience'] > 0.1]\n",
        "        for _, entity in high_salience_entities.iterrows():\n",
        "            entity_name = entity['Entity'].lower()\n",
        "            base_noise = ['cookie', 'site', 'privacy', 'policy']\n",
        "            config_noise = [term.lower() for term in self.noise_patterns]\n",
        "            all_noise = base_noise + config_noise\n",
        "\n",
        "            if any(noise in entity_name for noise in all_noise):\n",
        "                issues.append(f\"High-salience noise entity: '{entity['Entity']}' (salience: {entity['Salience']:.3f})\")\n",
        "\n",
        "        # Check for business entities classified as \"OTHER\"\n",
        "        client_terms = [word.lower() for word in self.client_name.split('_')]\n",
        "        other_entities = page_entities[page_entities['Type'] == 'OTHER']\n",
        "\n",
        "        for _, entity in other_entities.iterrows():\n",
        "            entity_name = entity['Entity'].lower()\n",
        "            if any(term in entity_name for term in client_terms) and entity['Salience'] > 0.05:\n",
        "                issues.append(f\"Business entity classified as OTHER: '{entity['Entity']}' (should be ORGANIZATION)\")\n",
        "\n",
        "        # Calculate entity quality score\n",
        "        total_entities = len(page_entities)\n",
        "        meaningful_entities = len(page_entities[page_entities['Salience'] > 0.02])\n",
        "        entity_quality_score = meaningful_entities / total_entities if total_entities > 0 else 0\n",
        "\n",
        "        # Get top entities\n",
        "        top_entities = page_entities.nlargest(5, 'Salience')[\n",
        "            ['Entity', 'Type', 'Salience', 'Sentiment Score']\n",
        "        ].to_dict('records')\n",
        "\n",
        "        return {\n",
        "            'total_entities': total_entities,\n",
        "            'meaningful_entities': meaningful_entities,\n",
        "            'entity_issues': issues,\n",
        "            'entity_quality_score': entity_quality_score,\n",
        "            'top_entities': top_entities\n",
        "        }\n",
        "\n",
        "    def analyze_page_comprehensively(self, url, content):\n",
        "        \"\"\"Comprehensive analysis of a single page with configuration\"\"\"\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = self.clean_content(content)\n",
        "\n",
        "        # Get entity data for this page if available\n",
        "        page_entities = self.get_page_entities(url)\n",
        "\n",
        "        # Analyze entity quality\n",
        "        entity_analysis = self.analyze_entity_quality(url, page_entities)\n",
        "\n",
        "        # Extract triples for each configured category\n",
        "        all_triples = []\n",
        "        category_scores = {}\n",
        "\n",
        "        for category_name in self.categories.keys():\n",
        "            triples = self.extract_category_triples(cleaned_content, category_name, page_entities)\n",
        "            all_triples.extend(triples)\n",
        "\n",
        "            # Calculate category score based on triples found\n",
        "            high_conf_triples = [t for t in triples if t['confidence'] > 0.6]\n",
        "            category_scores[category_name] = {\n",
        "                'total_triples': len(triples),\n",
        "                'high_confidence_triples': len(high_conf_triples),\n",
        "                'avg_confidence': np.mean([t['confidence'] for t in triples]) if triples else 0,\n",
        "                'score': min(1.0, len(high_conf_triples) / 3.0),\n",
        "                'weight': self.categories[category_name]['weight'],\n",
        "                'entity_enhanced_triples': len([t for t in triples if t.get('entity_enhanced', False)])\n",
        "            }\n",
        "\n",
        "        # Calculate overall page metrics\n",
        "        word_count = len(cleaned_content.split())\n",
        "\n",
        "        metrics = {\n",
        "            'url': url,\n",
        "            'url_short': self._get_url_short(url),\n",
        "            'client_name': self.client_name,\n",
        "            'analysis_mode': 'Enhanced (Entity + Text)' if self.entity_enhanced else 'Text-Only',\n",
        "            'original_content_length': len(content),\n",
        "            'cleaned_content_length': len(cleaned_content),\n",
        "            'noise_ratio': 1 - (len(cleaned_content) / len(content)) if content else 0,\n",
        "            'word_count': word_count,\n",
        "            'total_triples': len(all_triples),\n",
        "            'high_confidence_triples': len([t for t in all_triples if t['confidence'] > 0.6]),\n",
        "            'entity_enhanced_triples': len([t for t in all_triples if t.get('entity_enhanced', False)]),\n",
        "            'category_coverage': len([cat for cat, scores in category_scores.items() if scores['total_triples'] > 0]),\n",
        "            'category_scores': category_scores,\n",
        "            'semantic_triples': all_triples,\n",
        "            'entity_analysis': entity_analysis\n",
        "        }\n",
        "\n",
        "        # Calculate composite semantic quality score using configuration weights\n",
        "        metrics['semantic_quality_score'] = self._calculate_weighted_quality_score(category_scores, entity_analysis)\n",
        "\n",
        "        # Determine priority based on configuration thresholds\n",
        "        metrics['priority'] = self._determine_priority(\n",
        "            metrics['semantic_quality_score'],\n",
        "            metrics['category_coverage']\n",
        "        )\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _get_url_short(self, url):\n",
        "        \"\"\"Extract short URL identifier\"\"\"\n",
        "        parts = url.split('/')\n",
        "        if len(parts) > 3:\n",
        "            return parts[-2] if parts[-1] == '' else parts[-1]\n",
        "        return url.split('/')[-1] or url\n",
        "\n",
        "    def _calculate_weighted_quality_score(self, category_scores, entity_analysis):\n",
        "        \"\"\"Calculate weighted semantic quality score using configuration\"\"\"\n",
        "        base_score = 0\n",
        "\n",
        "        # Calculate weighted category scores\n",
        "        for category_name, scores in category_scores.items():\n",
        "            weight = scores['weight']\n",
        "            category_score = scores['score']\n",
        "            base_score += weight * category_score\n",
        "\n",
        "        # Add entity quality factor if entity analysis is available\n",
        "        if self.entity_enhanced and entity_analysis['total_entities'] > 0:\n",
        "            entity_quality_factor = entity_analysis['entity_quality_score'] * 0.1\n",
        "            base_score += entity_quality_factor\n",
        "\n",
        "        return min(1.0, base_score)\n",
        "\n",
        "    def _determine_priority(self, quality_score, category_coverage):\n",
        "        \"\"\"Determine priority based on configuration thresholds\"\"\"\n",
        "        high_threshold = self.priority_thresholds['high']\n",
        "        medium_threshold = self.priority_thresholds['medium']\n",
        "\n",
        "        if quality_score < high_threshold or category_coverage < 3:\n",
        "            return 'HIGH'\n",
        "        elif quality_score < medium_threshold or category_coverage < 4:\n",
        "            return 'MEDIUM'\n",
        "        else:\n",
        "            return 'LOW'\n",
        "\n",
        "# =====================================================\n",
        "# V2 ANALYSIS EXECUTION FUNCTIONS\n",
        "# =====================================================\n",
        "\n",
        "def analyze_with_configuration(text_file_path, client_config, entity_file_path=None):\n",
        "    \"\"\"Main analysis function using client configuration\"\"\"\n",
        "\n",
        "    print(f\"ðŸš€ Starting V2 Semantic Analysis\")\n",
        "    print(f\"ðŸ‘¤ Client: {client_config['client_name']}\")\n",
        "    print(f\"ðŸ“‹ Template: {client_config.get('template_used', 'Custom')}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize analyzer with configuration\n",
        "    analyzer = SemanticAnalyzerV2(client_config)\n",
        "\n",
        "    # Auto-detect and load entity data if available\n",
        "    if entity_file_path:\n",
        "        analyzer.load_entity_data(entity_file_path)\n",
        "\n",
        "    # Load text data\n",
        "    print(f\"ðŸ“– Loading text data from: {text_file_path}\")\n",
        "    text_df = pd.read_excel(text_file_path)\n",
        "    print(f\"âœ… Loaded {len(text_df)} pages for analysis\")\n",
        "\n",
        "    # Analyze all pages\n",
        "    results = []\n",
        "    print(\"\\nðŸ” Analyzing pages...\")\n",
        "\n",
        "    for index, row in text_df.iterrows():\n",
        "        url = row['URL']\n",
        "        content = row['content']\n",
        "\n",
        "        page_name = analyzer._get_url_short(url)\n",
        "        if index % 5 == 0 or index == len(text_df) - 1:\n",
        "            print(f\"   Processing page {index+1}/{len(text_df)}: {page_name}\")\n",
        "\n",
        "        # Analyze this page\n",
        "        analysis = analyzer.analyze_page_comprehensively(url, content)\n",
        "        results.append(analysis)\n",
        "\n",
        "    # Convert to DataFrame for easier analysis\n",
        "    results_df = pd.DataFrame([\n",
        "        {\n",
        "            'url_short': r['url_short'],\n",
        "            'client_name': r['client_name'],\n",
        "            'analysis_mode': r['analysis_mode'],\n",
        "            'semantic_quality_score': r['semantic_quality_score'],\n",
        "            'total_triples': r['total_triples'],\n",
        "            'high_confidence_triples': r['high_confidence_triples'],\n",
        "            'entity_enhanced_triples': r['entity_enhanced_triples'],\n",
        "            'category_coverage': r['category_coverage'],\n",
        "            'noise_ratio': r['noise_ratio'],\n",
        "            'priority': r['priority'],\n",
        "            'entity_quality_score': r['entity_analysis']['entity_quality_score'],\n",
        "            'entity_issues_count': len(r['entity_analysis']['entity_issues']),\n",
        "            **{f\"{cat}_score\": r['category_scores'][cat]['score'] for cat in client_config['categories'].keys()}\n",
        "        }\n",
        "        for r in results\n",
        "    ])\n",
        "\n",
        "    # Analysis summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ðŸ“Š ANALYSIS COMPLETE\")\n",
        "    print(f\"âœ… Pages analyzed: {len(results_df)}\")\n",
        "    print(f\"ðŸ“ˆ Average quality score: {results_df['semantic_quality_score'].mean():.3f}\")\n",
        "    print(f\"âš¡ Analysis mode: {results[0]['analysis_mode']}\")\n",
        "    print(f\"ðŸ”¥ Entity enhancement: {'YES' if analyzer.entity_enhanced else 'NO'}\")\n",
        "\n",
        "    # Priority breakdown\n",
        "    priority_counts = results_df['priority'].value_counts()\n",
        "    for priority, count in priority_counts.items():\n",
        "        print(f\"ðŸŽ¯ {priority} priority pages: {count}\")\n",
        "\n",
        "    return results, results_df, analyzer\n",
        "\n",
        "def save_analysis_results(results, results_df, client_config, analysis_folder):\n",
        "    \"\"\"Save analysis results to organized Drive folders\"\"\"\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Saving results to: {analysis_folder}\")\n",
        "\n",
        "    # Generate file names using client configuration\n",
        "    file_prefix = client_config[\"file_naming\"][\"analysis_prefix\"]\n",
        "\n",
        "    # Main results file\n",
        "    main_results_file = f\"{analysis_folder}/results/{file_prefix}_analysis_results.xlsx\"\n",
        "\n",
        "    with pd.ExcelWriter(main_results_file, engine='openpyxl') as writer:\n",
        "        # Summary results\n",
        "        results_df.to_excel(writer, sheet_name='Summary Results', index=False)\n",
        "\n",
        "        # Category breakdown\n",
        "        category_breakdown = []\n",
        "        for result in results:\n",
        "            for category, scores in result['category_scores'].items():\n",
        "                category_breakdown.append({\n",
        "                    'URL': result['url_short'],\n",
        "                    'Category': category.replace('_', ' ').title(),\n",
        "                    'Score': scores['score'],\n",
        "                    'Weight': scores['weight'],\n",
        "                    'Total Triples': scores['total_triples'],\n",
        "                    'High Confidence Triples': scores['high_confidence_triples']\n",
        "                })\n",
        "\n",
        "        category_df = pd.DataFrame(category_breakdown)\n",
        "        category_df.to_excel(writer, sheet_name='Category Analysis', index=False)\n",
        "\n",
        "        # All semantic triples\n",
        "        all_triples = []\n",
        "        for result in results:\n",
        "            for triple in result['semantic_triples']:\n",
        "                all_triples.append({\n",
        "                    'URL': result['url_short'],\n",
        "                    'Category': triple['category'].replace('_', ' ').title(),\n",
        "                    'Subject': triple['subject'],\n",
        "                    'Predicate': triple['predicate'],\n",
        "                    'Object': triple['object'],\n",
        "                    'Confidence': round(triple['confidence'], 3),\n",
        "                    'Entity Enhanced': 'Yes' if triple.get('entity_enhanced', False) else 'No'\n",
        "                })\n",
        "\n",
        "        if all_triples:\n",
        "            triples_df = pd.DataFrame(all_triples)\n",
        "            triples_df.to_excel(writer, sheet_name='Semantic Triples', index=False)\n",
        "\n",
        "    # Individual URL reports\n",
        "    url_reports_folder = f\"{analysis_folder}/results/individual_reports\"\n",
        "    os.makedirs(url_reports_folder, exist_ok=True)\n",
        "\n",
        "    for result in results:\n",
        "        url_file = f\"{url_reports_folder}/{result['url_short']}_analysis.xlsx\"\n",
        "\n",
        "        with pd.ExcelWriter(url_file, engine='openpyxl') as writer:\n",
        "            # Overview\n",
        "            overview_data = {\n",
        "                'Metric': ['URL', 'Quality Score', 'Priority', 'Analysis Mode', 'Category Coverage'],\n",
        "                'Value': [\n",
        "                    result['url'],\n",
        "                    round(result['semantic_quality_score'], 3),\n",
        "                    result['priority'],\n",
        "                    result['analysis_mode'],\n",
        "                    f\"{result['category_coverage']}/{len(client_config['categories'])}\"\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            overview_df = pd.DataFrame(overview_data)\n",
        "            overview_df.to_excel(writer, sheet_name='Overview', index=False)\n",
        "\n",
        "    print(f\"âœ… Results saved successfully!\")\n",
        "    print(f\"ðŸ“ Main file: {main_results_file}\")\n",
        "    print(f\"ðŸ“ Individual reports: {url_reports_folder}\")\n",
        "\n",
        "    return main_results_file\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE WORKFLOW FUNCTION\n",
        "# =====================================================\n",
        "\n",
        "def run_complete_analysis_v2(client_name, text_file_drive_path, entity_file_drive_path=None, config_manager=None):\n",
        "    \"\"\"Complete V2 analysis workflow\"\"\"\n",
        "\n",
        "    print(\"ðŸš€ SEMANTIC ANALYZER V2 - COMPLETE WORKFLOW\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load configuration\n",
        "    if not config_manager:\n",
        "        config_manager = ConfigurationManager()\n",
        "\n",
        "    client_config = config_manager.load_client_config(client_name)\n",
        "    if not client_config:\n",
        "        print(f\"âŒ Client configuration not found: {client_name}\")\n",
        "        print(\"Available clients:\", config_manager.list_available_configs())\n",
        "        return None\n",
        "\n",
        "    # Create analysis folder\n",
        "    analysis_folder = config_manager.create_analysis_folder(client_config)\n",
        "\n",
        "    # Construct full file paths\n",
        "    base_drive_path = \"/content/drive/MyDrive\"\n",
        "    text_file_path = f\"{base_drive_path}/{text_file_drive_path}\"\n",
        "    entity_file_path = f\"{base_drive_path}/{entity_file_drive_path}\" if entity_file_drive_path else None\n",
        "\n",
        "    # Run analysis\n",
        "    results, results_df, analyzer = analyze_with_configuration(\n",
        "        text_file_path,\n",
        "        client_config,\n",
        "        entity_file_path\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    main_file = save_analysis_results(results, results_df, client_config, analysis_folder)\n",
        "\n",
        "    print(\"\\nðŸŽ‰ V2 ANALYSIS COMPLETE!\")\n",
        "    print(f\"ðŸ“Š Ready for manual review and visualization\")\n",
        "\n",
        "    return results, results_df, analyzer, client_config, analysis_folder\n",
        "\n",
        "# =====================================================\n",
        "# READY TO USE\n",
        "# =====================================================\n",
        "\n",
        "print(\"âœ… V2 Semantic Analyzer Loaded!\")\n",
        "print(\"\\nQuick Start:\")\n",
        "print(\"1. First run Cell 1 to set up configuration\")\n",
        "print(\"2. Create client: client_config, folder, manager = create_new_client('ClientName', 'academic')\")\n",
        "print(\"3. Run analysis: results, results_df, analyzer, config, folder = run_complete_analysis_v2(\")\n",
        "print(\"   'ClientName', 'path/to/text_file.xlsx', 'path/to/entity_file.xlsx')\")"
      ],
      "metadata": {
        "id": "tbofPjn3W9zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: V2 Visualization Suite\n",
        "\n",
        "- Configuration-Aware Visualizations - Uses client categories and display names\n",
        "- Executive Dashboard - High-level metrics for stakeholders\n",
        "- Category Performance Analysis - Detailed breakdown of configured categories\n",
        "- Content Optimization Matrix - Interactive priority visualization\n",
        "- Semantic Triple Analysis - Deep dive into relationships found\n",
        "- Competitive Analysis - Competitor mentions and differentiation language\n",
        "- Flexible Execution - Run all or specific visualizations\n",
        "\n",
        "Key Features:\n",
        "\n",
        "- Uses academic template category names and structure\n",
        "- Shows entity enhancement impact when available\n",
        "- Adapts to any number of configured categories\n",
        "- Interactive Plotly charts for detailed exploration\n",
        "- Configuration-driven thresholds and competitor analysis"
      ],
      "metadata": {
        "id": "4OeiWfOdYgo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "# =====================================================\n",
        "# V2 VISUALIZATION SUITE\n",
        "# =====================================================\n",
        "\n",
        "class VisualizationSuiteV2:\n",
        "    \"\"\"Configuration-aware visualization suite for V2 results\"\"\"\n",
        "\n",
        "    def __init__(self, results, results_df, client_config, analyzer):\n",
        "        self.results = results\n",
        "        self.results_df = results_df\n",
        "        self.client_config = client_config\n",
        "        self.analyzer = analyzer\n",
        "        self.client_name = client_config['client_name']\n",
        "        self.categories = list(client_config['categories'].keys())\n",
        "        self.category_display_names = {\n",
        "            cat: config['display_name']\n",
        "            for cat, config in client_config['categories'].items()\n",
        "        }\n",
        "\n",
        "        print(f\"ðŸŽ¨ Visualization Suite initialized for: {self.client_name}\")\n",
        "        print(f\"ðŸ“Š Analysis mode: {results[0]['analysis_mode']}\")\n",
        "        print(f\"ðŸ† Categories: {len(self.categories)}\")\n",
        "\n",
        "    def create_executive_dashboard(self):\n",
        "        \"\"\"Create high-level executive dashboard\"\"\"\n",
        "\n",
        "        print(\"ðŸ“ˆ Creating Executive Dashboard...\")\n",
        "\n",
        "        # Calculate key metrics\n",
        "        total_pages = len(self.results_df)\n",
        "        avg_quality = self.results_df['semantic_quality_score'].mean()\n",
        "        high_priority = len(self.results_df[self.results_df['priority'] == 'HIGH'])\n",
        "        medium_priority = len(self.results_df[self.results_df['priority'] == 'MEDIUM'])\n",
        "        low_priority = len(self.results_df[self.results_df['priority'] == 'LOW'])\n",
        "\n",
        "        # Create executive metrics visualization\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "        # Key metrics boxes\n",
        "        metrics = [\n",
        "            ('Pages Analyzed', total_pages, 'blue'),\n",
        "            ('Avg Quality Score', f'{avg_quality:.3f}', 'green'),\n",
        "            ('High Priority Pages', high_priority, 'red'),\n",
        "            ('Medium Priority Pages', medium_priority, 'orange'),\n",
        "            ('Low Priority Pages', low_priority, 'darkgreen'),\n",
        "            ('Analysis Mode', self.results[0]['analysis_mode'], 'purple')\n",
        "        ]\n",
        "\n",
        "        for i, (label, value, color) in enumerate(metrics):\n",
        "            row, col = i // 3, i % 3\n",
        "            axes[row, col].text(0.5, 0.5, str(value), ha='center', va='center',\n",
        "                               fontsize=24, fontweight='bold', color=color)\n",
        "            axes[row, col].text(0.5, 0.2, label, ha='center', va='center',\n",
        "                               fontsize=12, wrap=True)\n",
        "            axes[row, col].set_xlim(0, 1)\n",
        "            axes[row, col].set_ylim(0, 1)\n",
        "            axes[row, col].axis('off')\n",
        "\n",
        "            # Add border\n",
        "            for spine in ['top', 'right', 'bottom', 'left']:\n",
        "                axes[row, col].spines[spine].set_visible(True)\n",
        "                axes[row, col].spines[spine].set_color(color)\n",
        "                axes[row, col].spines[spine].set_linewidth(2)\n",
        "\n",
        "        plt.suptitle(f'Semantic Analysis Executive Summary - {self.client_name}',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def create_category_performance_analysis(self):\n",
        "        \"\"\"Create category-specific performance analysis\"\"\"\n",
        "\n",
        "        print(\"ðŸ“Š Creating Category Performance Analysis...\")\n",
        "\n",
        "        # Prepare category data\n",
        "        category_columns = [f\"{cat}_score\" for cat in self.categories]\n",
        "        category_data = self.results_df[category_columns]\n",
        "\n",
        "        # Rename columns for display\n",
        "        display_columns = {}\n",
        "        for cat in self.categories:\n",
        "            display_name = self.category_display_names.get(cat, cat.replace('_', ' ').title())\n",
        "            display_columns[f\"{cat}_score\"] = display_name\n",
        "\n",
        "        category_data = category_data.rename(columns=display_columns)\n",
        "\n",
        "        # Create visualizations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # 1. Category Performance Radar Chart\n",
        "        categories_display = list(display_columns.values())\n",
        "        category_averages = [category_data[col].mean() for col in categories_display]\n",
        "\n",
        "        # Radar chart setup\n",
        "        angles = np.linspace(0, 2*np.pi, len(categories_display), endpoint=False).tolist()\n",
        "        category_averages += category_averages[:1]  # Complete the circle\n",
        "        angles += angles[:1]\n",
        "\n",
        "        axes[0,0].plot(angles, category_averages, 'o-', linewidth=2, color='blue', label='Average Score')\n",
        "        axes[0,0].fill(angles, category_averages, alpha=0.25, color='blue')\n",
        "        axes[0,0].set_xticks(angles[:-1])\n",
        "        axes[0,0].set_xticklabels(categories_display, fontsize=10)\n",
        "        axes[0,0].set_ylim(0, 1)\n",
        "        axes[0,0].set_title('Category Performance Overview')\n",
        "        axes[0,0].grid(True)\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # 2. Category Performance Bar Chart\n",
        "        avg_scores = category_data.mean().sort_values(ascending=True)\n",
        "        colors = ['red' if score < 0.3 else 'orange' if score < 0.6 else 'green' for score in avg_scores]\n",
        "\n",
        "        axes[0,1].barh(range(len(avg_scores)), avg_scores.values, color=colors)\n",
        "        axes[0,1].set_yticks(range(len(avg_scores)))\n",
        "        axes[0,1].set_yticklabels(avg_scores.index)\n",
        "        axes[0,1].set_xlabel('Average Score')\n",
        "        axes[0,1].set_title('Category Ranking (Lowest to Highest)')\n",
        "        axes[0,1].set_xlim(0, 1)\n",
        "\n",
        "        # Add score labels on bars\n",
        "        for i, score in enumerate(avg_scores.values):\n",
        "            axes[0,1].text(score + 0.02, i, f'{score:.3f}', va='center')\n",
        "\n",
        "        # 3. Category Distribution Heatmap\n",
        "        heatmap_data = category_data.T\n",
        "        sns.heatmap(heatmap_data, ax=axes[1,0], cmap='RdYlGn', center=0.5,\n",
        "                   cbar_kws={'label': 'Category Score'}, xticklabels=False)\n",
        "        axes[1,0].set_title('Category Scores by Page')\n",
        "        axes[1,0].set_ylabel('Categories')\n",
        "\n",
        "        # 4. Priority vs Category Coverage\n",
        "        priority_order = ['HIGH', 'MEDIUM', 'LOW']\n",
        "        priority_colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'green'}\n",
        "\n",
        "        for priority in priority_order:\n",
        "            priority_data = self.results_df[self.results_df['priority'] == priority]\n",
        "            if not priority_data.empty:\n",
        "                axes[1,1].scatter(priority_data['category_coverage'],\n",
        "                                priority_data['semantic_quality_score'],\n",
        "                                label=f'{priority} Priority',\n",
        "                                color=priority_colors[priority],\n",
        "                                alpha=0.7, s=50)\n",
        "\n",
        "        axes[1,1].set_xlabel('Category Coverage (out of 6)')\n",
        "        axes[1,1].set_ylabel('Semantic Quality Score')\n",
        "        axes[1,1].set_title('Priority Classification Analysis')\n",
        "        axes[1,1].legend()\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.suptitle(f'Category Performance Analysis - {self.client_name}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def create_content_optimization_matrix(self):\n",
        "        \"\"\"Create content optimization priority matrix\"\"\"\n",
        "\n",
        "        print(\"ðŸŽ¯ Creating Content Optimization Matrix...\")\n",
        "\n",
        "        # Create interactive priority matrix\n",
        "        fig = go.Figure()\n",
        "\n",
        "        priority_colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'green'}\n",
        "\n",
        "        for priority in ['HIGH', 'MEDIUM', 'LOW']:\n",
        "            priority_data = self.results_df[self.results_df['priority'] == priority]\n",
        "            if not priority_data.empty:\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=priority_data['total_triples'],\n",
        "                    y=priority_data['semantic_quality_score'],\n",
        "                    mode='markers+text',\n",
        "                    name=f'{priority} Priority',\n",
        "                    text=priority_data['url_short'],\n",
        "                    textposition='top center',\n",
        "                    marker=dict(\n",
        "                        color=priority_colors[priority],\n",
        "                        size=priority_data['category_coverage'] * 3 + 5,  # Size by category coverage\n",
        "                        opacity=0.7,\n",
        "                        line=dict(width=1, color='white')\n",
        "                    ),\n",
        "                    hovertemplate='<b>%{text}</b><br>' +\n",
        "                                'Quality Score: %{y:.3f}<br>' +\n",
        "                                'Total Triples: %{x}<br>' +\n",
        "                                'Priority: ' + priority + '<extra></extra>'\n",
        "                ))\n",
        "\n",
        "        # Add threshold lines\n",
        "        fig.add_hline(y=self.client_config['priority_thresholds']['high'],\n",
        "                     line_dash=\"dash\", line_color=\"red\",\n",
        "                     annotation_text=\"High Priority Threshold\")\n",
        "        fig.add_hline(y=self.client_config['priority_thresholds']['medium'],\n",
        "                     line_dash=\"dash\", line_color=\"orange\",\n",
        "                     annotation_text=\"Medium Priority Threshold\")\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f'Content Optimization Priority Matrix - {self.client_name}',\n",
        "            xaxis_title='Total Semantic Triples Found',\n",
        "            yaxis_title='Semantic Quality Score',\n",
        "            height=600,\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "    def create_semantic_triple_analysis(self):\n",
        "        \"\"\"Analyze semantic triples found across all pages\"\"\"\n",
        "\n",
        "        print(\"ðŸ” Creating Semantic Triple Analysis...\")\n",
        "\n",
        "        # Prepare triple data\n",
        "        all_triples = []\n",
        "        for result in self.results:\n",
        "            for triple in result['semantic_triples']:\n",
        "                all_triples.append({\n",
        "                    'URL': result['url_short'],\n",
        "                    'Category': self.category_display_names.get(\n",
        "                        triple['category'],\n",
        "                        triple['category'].replace('_', ' ').title()\n",
        "                    ),\n",
        "                    'Subject': triple['subject'],\n",
        "                    'Predicate': triple['predicate'],\n",
        "                    'Object': triple['object'],\n",
        "                    'Confidence': triple['confidence'],\n",
        "                    'Entity Enhanced': triple.get('entity_enhanced', False)\n",
        "                })\n",
        "\n",
        "        if not all_triples:\n",
        "            print(\"âš ï¸ No semantic triples found to analyze\")\n",
        "            return\n",
        "\n",
        "        triples_df = pd.DataFrame(all_triples)\n",
        "\n",
        "        # Create analysis visualizations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # 1. Triples by Category\n",
        "        category_counts = triples_df['Category'].value_counts()\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
        "\n",
        "        axes[0,0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
        "                     colors=colors, startangle=90)\n",
        "        axes[0,0].set_title('Semantic Triples Distribution by Category')\n",
        "\n",
        "        # 2. Confidence Score Distribution\n",
        "        axes[0,1].hist(triples_df['Confidence'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[0,1].axvline(triples_df['Confidence'].mean(), color='red', linestyle='--',\n",
        "                         label=f'Mean: {triples_df[\"Confidence\"].mean():.3f}')\n",
        "        axes[0,1].set_xlabel('Confidence Score')\n",
        "        axes[0,1].set_ylabel('Number of Triples')\n",
        "        axes[0,1].set_title('Semantic Triple Confidence Distribution')\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # 3. Entity Enhancement Impact\n",
        "        if self.analyzer.entity_enhanced:\n",
        "            enhancement_counts = triples_df['Entity Enhanced'].value_counts()\n",
        "            axes[1,0].bar(['Text Only', 'Entity Enhanced'],\n",
        "                         [enhancement_counts.get(False, 0), enhancement_counts.get(True, 0)],\n",
        "                         color=['lightblue', 'darkblue'])\n",
        "            axes[1,0].set_title('Entity Enhancement Impact')\n",
        "            axes[1,0].set_ylabel('Number of Triples')\n",
        "        else:\n",
        "            axes[1,0].text(0.5, 0.5, 'Text-Only Analysis\\nNo Entity Enhancement',\n",
        "                          ha='center', va='center', fontsize=14)\n",
        "            axes[1,0].set_title('Analysis Mode')\n",
        "\n",
        "        # 4. Top Predicates Used\n",
        "        predicate_counts = triples_df['Predicate'].value_counts().head(10)\n",
        "        axes[1,1].barh(range(len(predicate_counts)), predicate_counts.values, color='coral')\n",
        "        axes[1,1].set_yticks(range(len(predicate_counts)))\n",
        "        axes[1,1].set_yticklabels(predicate_counts.index)\n",
        "        axes[1,1].set_xlabel('Frequency')\n",
        "        axes[1,1].set_title('Most Common Relationship Types (Predicates)')\n",
        "\n",
        "        plt.suptitle(f'Semantic Triple Analysis - {self.client_name}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Show sample high-confidence triples\n",
        "        print(\"\\nðŸ† High-Confidence Semantic Triples (Sample):\")\n",
        "        high_conf_triples = triples_df[triples_df['Confidence'] > 0.7].head(10)\n",
        "        for _, triple in high_conf_triples.iterrows():\n",
        "            enhancement = \" ðŸ”¥\" if triple['Entity Enhanced'] else \"\"\n",
        "            print(f\"   [{triple['Category']}] {triple['Subject']} â†’ {triple['Predicate']} â†’ {triple['Object']}{enhancement}\")\n",
        "            print(f\"   Confidence: {triple['Confidence']:.3f}\")\n",
        "\n",
        "    def create_competitive_analysis(self):\n",
        "        \"\"\"Analyze competitor mentions and differentiation\"\"\"\n",
        "\n",
        "        print(\"ðŸ† Creating Competitive Analysis...\")\n",
        "\n",
        "        # Find competitor mentions in triples\n",
        "        competitors = self.client_config.get('competitors', [])\n",
        "        if not competitors:\n",
        "            print(\"âš ï¸ No competitors configured for analysis\")\n",
        "            return\n",
        "\n",
        "        competitor_mentions = []\n",
        "        differentiation_triples = []\n",
        "\n",
        "        for result in self.results:\n",
        "            url_short = result['url_short']\n",
        "\n",
        "            # Look for competitor mentions\n",
        "            for triple in result['semantic_triples']:\n",
        "                text_content = f\"{triple['subject']} {triple['predicate']} {triple['object']}\".lower()\n",
        "\n",
        "                for competitor in competitors:\n",
        "                    if competitor.lower() in text_content:\n",
        "                        competitor_mentions.append({\n",
        "                            'URL': url_short,\n",
        "                            'Competitor': competitor,\n",
        "                            'Context': f\"{triple['subject']} {triple['predicate']} {triple['object']}\",\n",
        "                            'Category': triple['category'],\n",
        "                            'Confidence': triple['confidence']\n",
        "                        })\n",
        "\n",
        "                # Look for differentiation language\n",
        "                diff_keywords = ['unlike', 'different', 'better', 'superior', 'unique', 'only', 'exclusively']\n",
        "                if any(keyword in text_content for keyword in diff_keywords):\n",
        "                    differentiation_triples.append({\n",
        "                        'URL': url_short,\n",
        "                        'Triple': f\"{triple['subject']} â†’ {triple['predicate']} â†’ {triple['object']}\",\n",
        "                        'Confidence': triple['confidence']\n",
        "                    })\n",
        "\n",
        "        # Create visualizations\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # 1. Competitor Mentions\n",
        "        if competitor_mentions:\n",
        "            comp_df = pd.DataFrame(competitor_mentions)\n",
        "            comp_counts = comp_df['Competitor'].value_counts()\n",
        "\n",
        "            axes[0].bar(range(len(comp_counts)), comp_counts.values, color='lightcoral')\n",
        "            axes[0].set_xticks(range(len(comp_counts)))\n",
        "            axes[0].set_xticklabels(comp_counts.index, rotation=45)\n",
        "            axes[0].set_ylabel('Number of Mentions')\n",
        "            axes[0].set_title('Competitor Mentions in Content')\n",
        "        else:\n",
        "            axes[0].text(0.5, 0.5, 'No Competitor\\nMentions Found', ha='center', va='center', fontsize=14)\n",
        "            axes[0].set_title('Competitor Analysis')\n",
        "\n",
        "        # 2. Differentiation Statements\n",
        "        if differentiation_triples:\n",
        "            diff_by_url = pd.DataFrame(differentiation_triples)['URL'].value_counts().head(10)\n",
        "\n",
        "            axes[1].barh(range(len(diff_by_url)), diff_by_url.values, color='lightgreen')\n",
        "            axes[1].set_yticks(range(len(diff_by_url)))\n",
        "            axes[1].set_yticklabels(diff_by_url.index)\n",
        "            axes[1].set_xlabel('Number of Differentiation Statements')\n",
        "            axes[1].set_title('Pages with Most Differentiation Language')\n",
        "        else:\n",
        "            axes[1].text(0.5, 0.5, 'No Differentiation\\nLanguage Found', ha='center', va='center', fontsize=14)\n",
        "            axes[1].set_title('Differentiation Analysis')\n",
        "\n",
        "        plt.suptitle(f'Competitive & Differentiation Analysis - {self.client_name}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print sample findings\n",
        "        if competitor_mentions:\n",
        "            print(f\"\\nðŸ” Competitor Mentions Found: {len(competitor_mentions)}\")\n",
        "            for mention in competitor_mentions[:5]:\n",
        "                print(f\"   {mention['URL']}: {mention['Context']}\")\n",
        "\n",
        "        if differentiation_triples:\n",
        "            print(f\"\\nðŸ’ª Differentiation Statements Found: {len(differentiation_triples)}\")\n",
        "            for diff in differentiation_triples[:5]:\n",
        "                print(f\"   {diff['URL']}: {diff['Triple']}\")\n",
        "\n",
        "# =====================================================\n",
        "# MAIN VISUALIZATION EXECUTION FUNCTIONS\n",
        "# =====================================================\n",
        "\n",
        "def run_complete_visualization_suite(results, results_df, client_config, analyzer):\n",
        "    \"\"\"Run the complete V2 visualization suite\"\"\"\n",
        "\n",
        "    print(\"ðŸŽ¨ STARTING V2 VISUALIZATION SUITE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize visualization suite\n",
        "    viz_suite = VisualizationSuiteV2(results, results_df, client_config, analyzer)\n",
        "\n",
        "    # Create all visualizations\n",
        "    print(\"\\n1. Executive Dashboard\")\n",
        "    viz_suite.create_executive_dashboard()\n",
        "\n",
        "    print(\"\\n2. Category Performance Analysis\")\n",
        "    viz_suite.create_category_performance_analysis()\n",
        "\n",
        "    print(\"\\n3. Content Optimization Matrix\")\n",
        "    viz_suite.create_content_optimization_matrix()\n",
        "\n",
        "    print(\"\\n4. Semantic Triple Analysis\")\n",
        "    viz_suite.create_semantic_triple_analysis()\n",
        "\n",
        "    print(\"\\n5. Competitive Analysis\")\n",
        "    viz_suite.create_competitive_analysis()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ðŸŽ‰ VISUALIZATION SUITE COMPLETE!\")\n",
        "    print(\"ðŸ“Š All charts generated successfully\")\n",
        "\n",
        "    return viz_suite\n",
        "\n",
        "def create_custom_visualization(results, results_df, client_config, viz_type=\"executive\"):\n",
        "    \"\"\"Create specific visualization type\"\"\"\n",
        "\n",
        "    viz_suite = VisualizationSuiteV2(results, results_df, client_config, analyzer)\n",
        "\n",
        "    if viz_type == \"executive\":\n",
        "        viz_suite.create_executive_dashboard()\n",
        "    elif viz_type == \"categories\":\n",
        "        viz_suite.create_category_performance_analysis()\n",
        "    elif viz_type == \"optimization\":\n",
        "        viz_suite.create_content_optimization_matrix()\n",
        "    elif viz_type == \"triples\":\n",
        "        viz_suite.create_semantic_triple_analysis()\n",
        "    elif viz_type == \"competitive\":\n",
        "        viz_suite.create_competitive_analysis()\n",
        "    else:\n",
        "        print(f\"âŒ Unknown visualization type: {viz_type}\")\n",
        "        print(\"Available types: executive, categories, optimization, triples, competitive\")\n",
        "\n",
        "# =====================================================\n",
        "# READY TO USE\n",
        "# =====================================================\n",
        "\n",
        "print(\"âœ… V2 Visualization Suite Loaded!\")\n",
        "print(\"\\nUsage after running Cell 2 analysis:\")\n",
        "print(\"1. Complete suite: viz_suite = run_complete_visualization_suite(results, results_df, client_config, analyzer)\")\n",
        "print(\"2. Specific chart: create_custom_visualization(results, results_df, client_config, 'executive')\")\n",
        "print(\"\\nVisualization types available:\")\n",
        "print(\"- executive: High-level dashboard\")\n",
        "print(\"- categories: Category performance analysis\")\n",
        "print(\"- optimization: Priority matrix for content optimization\")\n",
        "print(\"- triples: Semantic relationship analysis\")\n",
        "print(\"- competitive: Competitor and differentiation analysis\")"
      ],
      "metadata": {
        "id": "HHkr4zxMYd_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}